{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import functools\n",
    "from functional import compose, partial\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import plot\n",
    "import networks as nw\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utils and layers\n",
    "def compose_all(*args):\n",
    "    return partial(functools.reduce, compose)(*args)\n",
    "\n",
    "def print_(var, name: str, first_n=5, summarize=5):\n",
    "    \"\"\"Util for debugging, by printing values of tf.Variable `var` during training\"\"\"\n",
    "    # (tf.Tensor, str, int, int) -> tf.Tensor\n",
    "    return tf.Print(var, [var], \"{}: \".format(name), first_n=first_n,\n",
    "                    summarize=summarize)\n",
    "\n",
    "def get_mnist(n, mnist):\n",
    "    \"\"\"Returns 784-D numpy array for random MNIST digit `n`\"\"\"\n",
    "    assert 0 <= n <= 9, \"Must specify digit 0 - 9!\"\n",
    "    import random\n",
    "\n",
    "    SIZE = 500\n",
    "    imgs, labels = mnist.train.next_batch(SIZE)\n",
    "    idxs = iter(random.sample(range(SIZE), SIZE)) # non-in-place shuffle\n",
    "\n",
    "    for i in idxs:\n",
    "        if labels[i] == n:\n",
    "            return imgs[i] # first match\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_DIM = 28\n",
    "\n",
    "ARCHITECTURE = [IMG_DIM**2, # 784 pixels\n",
    "                500, 500, # intermediate encoding\n",
    "                2] # latent space dims\n",
    "                # 50]\n",
    "# (and symmetrically back out again)\n",
    "\n",
    "HYPERPARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"learning_rate\": 5E-4,\n",
    "    \"dropout\": 0.9,\n",
    "    \"lambda_l2_reg\": 1E-5,\n",
    "    \"nonlinearity\": tf.nn.elu,\n",
    "    \"squashing\": tf.nn.sigmoid\n",
    "}\n",
    "\n",
    "MAX_ITER = 2000#2**16\n",
    "MAX_EPOCHS = np.inf\n",
    "\n",
    "LOG_DIR = \"./log\"\n",
    "METAGRAPH_DIR = \"./out\"\n",
    "PLOTS_DIR = \"./png\"\n",
    "\n",
    "\n",
    "def load_mnist():\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    return input_data.read_data_sets(\"./mnist_data\")\n",
    "\n",
    "def all_plots(model, mnist):\n",
    "    if model.architecture[-1] == 2: # only works for 2-D latent\n",
    "        print(\"Plotting in latent space...\")\n",
    "        plot_all_in_latent(model, mnist)\n",
    "\n",
    "        print(\"Exploring latent...\")\n",
    "        plot.exploreLatent(model, nx=20, ny=20, range_=(-4, 4), outdir=PLOTS_DIR)\n",
    "        for n in (24, 30, 60, 100):\n",
    "            plot.exploreLatent(model, nx=n, ny=n, ppf=True, outdir=PLOTS_DIR,\n",
    "                               name=\"explore_ppf{}\".format(n))\n",
    "\n",
    "    print(\"Interpolating...\")\n",
    "    interpolate_digits(model, mnist)\n",
    "\n",
    "    print(\"Plotting end-to-end reconstructions...\")\n",
    "    plot_all_end_to_end(model, mnist)\n",
    "\n",
    "    print(\"Morphing...\")\n",
    "    morph_numbers(model, mnist, ns=[9,8,7,6,5,4,3,2,1,0])\n",
    "\n",
    "    print(\"Plotting 10 MNIST digits...\")\n",
    "    for i in range(10):\n",
    "        plot.justMNIST(get_mnist(i, mnist), name=str(i), outdir=PLOTS_DIR)\n",
    "\n",
    "def plot_all_in_latent(model, mnist):\n",
    "    names = (\"train\", \"validation\", \"test\")\n",
    "    datasets = (mnist.train, mnist.validation, mnist.test)\n",
    "    for name, dataset in zip(names, datasets):\n",
    "        plot.plotInLatent(model, dataset.images, dataset.labels, name=name,\n",
    "                          outdir=PLOTS_DIR)\n",
    "\n",
    "def interpolate_digits(model, mnist):\n",
    "    imgs, labels = mnist.train.next_batch(100)\n",
    "    idxs = np.random.randint(0, imgs.shape[0] - 1, 2)\n",
    "    mus, _ = model.encode(np.vstack(imgs[i] for i in idxs))\n",
    "    plot.interpolate(model, *mus, name=\"interpolate_{}->{}\".format(\n",
    "        *(labels[i] for i in idxs)), outdir=PLOTS_DIR)\n",
    "\n",
    "def plot_all_end_to_end(model, mnist):\n",
    "    names = (\"train\", \"validation\", \"test\")\n",
    "    datasets = (mnist.train, mnist.validation, mnist.test)\n",
    "    for name, dataset in zip(names, datasets):\n",
    "        x, _ = dataset.next_batch(10)\n",
    "        x_reconstructed = model.vae(x)\n",
    "        plot.plotSubset(model, x, x_reconstructed, n=10, name=name,\n",
    "                        outdir=PLOTS_DIR)\n",
    "\n",
    "def morph_numbers(model, mnist, ns=None, n_per_morph=10):\n",
    "    if not ns:\n",
    "        import random\n",
    "        ns = random.sample(range(10), 10) # non-in-place shuffle\n",
    "\n",
    "    xs = np.squeeze([get_mnist(n, mnist) for n in ns])\n",
    "    mus, _ = model.encode(xs)\n",
    "    plot.morph(model, mus, n_per_morph=n_per_morph, outdir=PLOTS_DIR,\n",
    "               name=\"morph_{}\".format(\"\".join(str(n) for n in ns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'decoding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-27e5d3afd54f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARCHITECTURE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHYPERPARAMS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOG_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     v.train(mnist, max_iter=MAX_ITER, max_epochs=MAX_EPOCHS, cross_validate=False,\n\u001b[1;32m     36\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMETAGRAPH_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplots_outdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPLOTS_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/earneodo/repos/playground/networks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, architecture, d_hyperparams, meta_graph, save_graph_def, log_dir)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m# build the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mhandles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRESTORE_KEY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/earneodo/repos/playground/networks.py\u001b[0m in \u001b[0;36m_build_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     for hidden_size in self.architecture[1:-1]]\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# final reconstruction: restore original dims, squash outputs [0, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mdecoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x_decoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marchitecture\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquashing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mx_reconstructed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompose_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'x_reconstructed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decoding' is not defined"
     ]
    }
   ],
   "source": [
    "IMG_DIM = 28\n",
    "\n",
    "ARCHITECTURE = [IMG_DIM**2, # 784 pixels\n",
    "                500, 500, # intermediate encoding\n",
    "                2] # latent space dims\n",
    "                # 50]\n",
    "# (and symmetrically back out again)\n",
    "\n",
    "HYPERPARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"learning_rate\": 5E-4,\n",
    "    \"dropout\": 0.9,\n",
    "    \"lambda_l2_reg\": 1E-5,\n",
    "    \"nonlinearity\": tf.nn.elu,\n",
    "    \"squashing\": tf.nn.sigmoid\n",
    "}\n",
    "\n",
    "MAX_ITER = 2000#2**16\n",
    "MAX_EPOCHS = np.inf\n",
    "\n",
    "LOG_DIR = \"./log\"\n",
    "METAGRAPH_DIR = \"./out\"\n",
    "PLOTS_DIR = \"./png\"\n",
    "\n",
    "mnist = load_mnist()\n",
    "to_reload = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(VAE)\n",
    "if to_reload: # restore\n",
    "    v = nw.VAE(ARCHITECTURE, HYPERPARAMS, meta_graph=to_reload)\n",
    "    print(\"Loaded!\")\n",
    "\n",
    "else: # train\n",
    "    v = nw.VAE(ARCHITECTURE, HYPERPARAMS, log_dir=LOG_DIR)\n",
    "    v.train(mnist, max_iter=MAX_ITER, max_epochs=MAX_EPOCHS, cross_validate=False,\n",
    "            verbose=True, save=True, outdir=METAGRAPH_DIR, plots_outdir=PLOTS_DIR,\n",
    "            plot_latent_over_time=False)\n",
    "    print(\"Trained!\")\n",
    "\n",
    "all_plots(v, mnist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tf 1.2 py3.6",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
