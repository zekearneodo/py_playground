{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A simple autoencoder from scratch using the contrib layers of tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import socket\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(x_, x, offset=1e-7):\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        x__ = tf.clip_by_value(x_, offset, 1 - offset)\n",
    "        return -tf.reduce_sum(x * tf.log(x__) + (1 - x) * tf.log(1 - x_), 1)\n",
    "    \n",
    "def kl_distance(mu, log_sigma):\n",
    "    with tf.name_scope('KL_divergence'):\n",
    "        return -0.5 * tf.reduce_sum(1 + 2 * log_sigma - mu**2 - tf.exp(2 * log_sigma), 1)\n",
    "        \n",
    "class VariationalAutoEncoder(object):\n",
    "    DEFAULTS = {\n",
    "        \"batch_size\": 128,\n",
    "        \"learning_rate\": 1E-3,\n",
    "        \"dropout\": 0.,\n",
    "        \"lambda_l2_reg\": 0.,\n",
    "        \"nonlinearity\": tf.nn.elu,\n",
    "        \"squashing\": tf.nn.sigmoid,\n",
    "        \"regularization\": tf.contrib.layers.l2_regularizer,\n",
    "        \"mu\": 0,\n",
    "        \"sigma\": 0.1,\n",
    "    }\n",
    "    RESTORE_KEY = \"to_restore\"\n",
    "    \n",
    "    def __init__(self, architecture=[], d_hyperparams={}, log_dir='./log'):\n",
    "        self.architecture = architecture\n",
    "        self.__dict__.update(VariationalAutoEncoder.DEFAULTS, **d_hyperparams)\n",
    "        self.sesh = tf.Session()\n",
    "        #TODO: decide if load a model or build a new one. For now, build it\n",
    "        handles = self._build_graph()\n",
    "        \n",
    "        # In any case, make a collection of variables that are restore-able\n",
    "        for handle in handles:\n",
    "            tf.add_to_collection(VariationalAutoEncoder.RESTORE_KEY, handle)\n",
    "            \n",
    "        self.sesh.run(tf.global_variables_initializer())\n",
    "\n",
    "        \n",
    "        # Unpack the tuple of handles created by the builder\n",
    "        (self.x_in, self.z_mean, self.z_log_sigma, self.x_reconstructed, self.z, \n",
    "               self.cost, self.global_step, self.train_op, self.merged_summaries, self.x0) = handles\n",
    "        \n",
    "        # Initialize the filewriter and write the graph (tensorboard)\n",
    "        \n",
    "        self.writer = tf.summary.FileWriter(log_dir, self.sesh.graph)\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        # The architecture is definded here, using contrib layers\n",
    "        # Tdodo: implement a compose_layers\n",
    "        \n",
    "        # the encoder\n",
    "        x_in = tf.placeholder(tf.float32, shape=[None, self.architecture[0]], name='x_in')\n",
    "        x0 = tf.layers.dense(x_in, 500, activation=self.nonlinearity, \n",
    "                            kernel_regularizer=self.regularization(self.lambda_l2_reg),\n",
    "                            name='enc_0')\n",
    "        x = tf.layers.dropout(x0, rate=self.dropout, name='drop_e0')\n",
    "        x = tf.layers.dense(x, 500, activation=self.nonlinearity, \n",
    "                            kernel_regularizer=self.regularization(self.lambda_l2_reg),\n",
    "                            name='enc_1')\n",
    "        x = tf.layers.dense(x, 100, activation=self.nonlinearity, \n",
    "                            kernel_regularizer=self.regularization(self.lambda_l2_reg),\n",
    "                            name='enc_2')\n",
    "#         for i, dim in enumerate(self.architecture[1:-1]):\n",
    "#             print('Encoder: Creating fully connected layer with sizes: IN {} - SIZE {}'.format(x.shape, dim))\n",
    "#             x = tf.layers.dense(x_in, dim, activation=self.nonlinearity, name='enc_{}'.format(i))\n",
    "        \n",
    "        # the latent space (separate the mean and the sigma)\n",
    "        z_mean = tf.layers.dense(x, self.architecture[-1], activation=self.nonlinearity, name='z_mean') \n",
    "        z_log_sigma = tf.layers.dense(x, self.architecture[-1], activation=self.nonlinearity, name='z_log_sigma')\n",
    "        print('Creating fully connected latent layers with sizes {}'.format(z_mean.shape))\n",
    "\n",
    "        # The sample from the latent space with n(0,1)\n",
    "        # With a little trick. Rathern than sampling N(z_mean, z_sigma), \n",
    "        # add a gaussian noise\n",
    "        eps = tf.random_normal(tf.shape(z_log_sigma), 0, self.sigma, \n",
    "                               dtype=tf.float32, name='eps')\n",
    "        \n",
    "        z = tf.add(z_mean, eps * tf.exp(z_log_sigma), name='z')\n",
    "        \n",
    "        # decoder (from z to x_out)\n",
    "        h = tf.layers.dense(z, 100, activation=self.nonlinearity, \n",
    "                            kernel_regularizer=self.regularization(self.lambda_l2_reg),\n",
    "                            name='dec_{}'.format(2))\n",
    "        h = tf.layers.dense(h, 500, activation=self.nonlinearity, \n",
    "                            kernel_regularizer=self.regularization(self.lambda_l2_reg),\n",
    "                            name='dec_{}'.format(1))\n",
    "        h = tf.layers.dropout(h, rate=self.dropout, name='drop_d0')\n",
    "        h = tf.layers.dense(h, 500, activation=self.nonlinearity, \n",
    "                            kernel_regularizer=self.regularization(self.lambda_l2_reg),\n",
    "                            name='dec_{}'.format(0))\n",
    "#         for i, dim in enumerate(self.architecture[1:-1][::-1]):\n",
    "#             print('Decoder: Creating fully connected layer with sizes: IN {} - SIZE {}'.format(h.shape, dim))\n",
    "#             h = tf.layers.dense(h, dim, activation=self.nonlinearity, name='dec_{}'.format(i))\n",
    "        print('Created decoder layers')\n",
    "        # final reconstruction with squashing to [0, 1]\n",
    "        y = tf.layers.dense(h, self.architecture[0], activation=self.squashing, \n",
    "                            kernel_regularizer=self.regularization(self.lambda_l2_reg),\n",
    "                            name='decoder')\n",
    "        x_reconstructed = tf.identity(y, name='x_reconstructed')\n",
    "        \n",
    "        # reconstruction loss: cross-entropy\n",
    "        rec_loss = cross_entropy(x_reconstructed, x_in)\n",
    "        \n",
    "        # KL divergence: information loss btw X and Z representations\n",
    "        kl_loss = kl_distance(z_mean, z_log_sigma)\n",
    "        \n",
    "        # regularization loss\n",
    "        l2_loss = tf.losses.get_regularization_loss()\n",
    "        \n",
    "        with tf.name_scope('cost'):\n",
    "            cost = tf.reduce_sum(rec_loss + kl_loss, name='vae_cost')\n",
    "            \n",
    "        # append to summary\n",
    "        with tf.name_scope('summaries'):\n",
    "            with tf.name_scope('loss'):\n",
    "                tf.summary.scalar('kl_loss', tf.reduce_sum(kl_loss, name='kl_scalar'))\n",
    "                tf.summary.scalar('rec_loss', tf.reduce_sum(rec_loss, name='rl_scalar'))\n",
    "                tf.summary.scalar('cost', cost)\n",
    "                tf.summary.scalar('l2_loss', l2_loss)\n",
    "                tf.summary.histogram('kl_hist', kl_loss)\n",
    "        \n",
    "        print('created all layers')\n",
    "        # The optimization\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        with tf.name_scope('AdamOptimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            # simple optimization\n",
    "            # train_op = optimizer.minimize(cost, global_step=global_step, name='minimize_cost')\n",
    "            # With clipped gradients:\n",
    "            train_vars = tf.trainable_variables()\n",
    "            grads_and_vars = optimizer.compute_gradients(cost, var_list=train_vars)\n",
    "            \n",
    "            clipped = []\n",
    "            for grad, var in grads_and_vars:\n",
    "                print('var {}'.format(var))\n",
    "                print('grad {}'.format(grad))\n",
    "                # add histograms of gradients, per variable (?)\n",
    "#                 with tf.name_scope('summaries'):\n",
    "#                     with tf.name_scope('gradients'):\n",
    "#                         tf.summary.histogram(grad)\n",
    "                clipped.append((tf.clip_by_value(grad, -5, 5), var))\n",
    "#             clipped = [(tf.clip_by_value(grad, -5, 5), var) for \n",
    "#                       grad, var in grads_and_vars]\n",
    "            train_op = optimizer.apply_gradients(clipped, global_step=global_step, \n",
    "                                                name='minimize_cost')\n",
    "        \n",
    "        merged_summaries = tf.summary.merge_all()\n",
    "        # return tuple of operations\n",
    "        return(x_in, z_mean, z_log_sigma, x_reconstructed, z, \n",
    "               cost, global_step, train_op, merged_summaries, x0)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        feed_dict = {self.x_in: x}\n",
    "        return self.sesh.run([self.z_mean, self.z_log_sigma], feed_dict=feed_dict)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        feed_dict = {self.z: z}\n",
    "        return self.sesh.run(self.x_reconstructed, feed_dict=feed_dict)\n",
    "    \n",
    "    def vae(self, x):\n",
    "        feed_dict = {self.x_in: x}\n",
    "        return self.sesh.run(self.x_reconstructed, feed_dict=feed_dict)\n",
    "    \n",
    "    def train(self, X, max_iter=np.inf, max_epochs=np.inf, cross_validate=True, verbose=True):\n",
    "        try:\n",
    "            err_train = 0\n",
    "            \n",
    "            while True:\n",
    "                x, _ = X.train.next_batch(self.batch_size)\n",
    "                feed_dict = {self.x_in: x}\n",
    "                fetches = [self.x_reconstructed, self.cost, self.global_step, self.train_op]\n",
    "                x_reconstructed, cost, i, _ = self.sesh.run(fetches, feed_dict=feed_dict)\n",
    "\n",
    "                err_train += cost\n",
    "                if i % 10 == 0:  # Record summaries and test-set accuracy\n",
    "                    summary = self.sesh.run(self.merged_summaries, feed_dict=feed_dict)\n",
    "                    self.writer.add_summary(summary, i)\n",
    "                if i%1000 and verbose:\n",
    "                    print('Round {}, loss {}'.format(i, cost))\n",
    "                if i>=max_iter or X.train.epochs_completed >= max_epochs:\n",
    "                    print(\"final avg cost (@ step {} = epoch {}): {}\".format(\n",
    "                            i, X.train.epochs_completed, err_train / i))\n",
    "                    try:\n",
    "                        self.writer.flush()\n",
    "                        self.writer.close()\n",
    "                    except(AttributeError):  # not logging\n",
    "                        continue\n",
    "                    break\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"final avg cost (@ step {} = epoch {}): {}\".format(\n",
    "                i, X.train.epochs_completed, err_train / i))\n",
    "            print(\"------- Training end: {} -------\\n\".format(now))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use it with the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    return input_data.read_data_sets(\"./mnist_data\")\n",
    "\n",
    "def all_plots(model, mnist):\n",
    "    if model.architecture[-1] == 2: # only works for 2-D latent\n",
    "        print(\"Plotting in latent space...\")\n",
    "        plot_all_in_latent(model, mnist)\n",
    "\n",
    "        print(\"Exploring latent...\")\n",
    "        plot.exploreLatent(model, nx=20, ny=20, range_=(-4, 4), outdir=PLOTS_DIR)\n",
    "        for n in (24, 30, 60, 100):\n",
    "            plot.exploreLatent(model, nx=n, ny=n, ppf=True, outdir=PLOTS_DIR,\n",
    "                               name=\"explore_ppf{}\".format(n))\n",
    "\n",
    "    print(\"Interpolating...\")\n",
    "    interpolate_digits(model, mnist)\n",
    "\n",
    "    print(\"Plotting end-to-end reconstructions...\")\n",
    "    plot_all_end_to_end(model, mnist)\n",
    "\n",
    "    print(\"Morphing...\")\n",
    "    morph_numbers(model, mnist, ns=[9,8,7,6,5,4,3,2,1,0])\n",
    "\n",
    "    print(\"Plotting 10 MNIST digits...\")\n",
    "    for i in range(10):\n",
    "        plot.justMNIST(get_mnist(i, mnist), name=str(i), outdir=PLOTS_DIR)\n",
    "\n",
    "def plot_all_in_latent(model, mnist):\n",
    "    names = (\"train\", \"validation\", \"test\")\n",
    "    datasets = (mnist.train, mnist.validation, mnist.test)\n",
    "    for name, dataset in zip(names, datasets):\n",
    "        plot.plotInLatent(model, dataset.images, dataset.labels, name=name,\n",
    "                          outdir=PLOTS_DIR)\n",
    "        \n",
    "def interpolate_digits(model, mnist):\n",
    "    imgs, labels = mnist.train.next_batch(100)\n",
    "    idxs = np.random.randint(0, imgs.shape[0] - 1, 2)\n",
    "    mus, _ = model.encode(np.vstack(imgs[i] for i in idxs))\n",
    "    plot.interpolate(model, *mus, name=\"interpolate_{}->{}\".format(\n",
    "        *(labels[i] for i in idxs)), outdir=PLOTS_DIR)\n",
    "\n",
    "def plot_all_end_to_end(model, mnist):\n",
    "    names = (\"train\", \"validation\", \"test\")\n",
    "    datasets = (mnist.train, mnist.validation, mnist.test)\n",
    "    for name, dataset in zip(names, datasets):\n",
    "        x, _ = dataset.next_batch(10)\n",
    "        x_reconstructed = model.vae(x)\n",
    "        plot.plotSubset(model, x, x_reconstructed, n=10, name=name,\n",
    "                        outdir=PLOTS_DIR)\n",
    "\n",
    "def morph_numbers(model, mnist, ns=None, n_per_morph=10):\n",
    "    if not ns:\n",
    "        import random\n",
    "        ns = random.sample(range(10), 10) # non-in-place shuffle\n",
    "\n",
    "    xs = np.squeeze([get_mnist(n, mnist) for n in ns])\n",
    "    mus, _ = model.encode(xs)\n",
    "    plot.morph(model, mus, n_per_morph=n_per_morph, outdir=PLOTS_DIR,\n",
    "               name=\"morph_{}\".format(\"\".join(str(n) for n in ns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_DIM = 28\n",
    "\n",
    "ARCHITECTURE = [IMG_DIM**2, # 784 pixels\n",
    "                500, 500, # intermediate encoding\n",
    "                2] # latent space dims\n",
    "                # 50]\n",
    "# (and symmetrically back out again)\n",
    "\n",
    "HYPERPARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"learning_rate\": 5E-4,\n",
    "    \"dropout\": 0.9,\n",
    "    \"lambda_l2_reg\": 1E-5,\n",
    "    \"nonlinearity\": tf.nn.elu,\n",
    "    \"squashing\": tf.nn.sigmoid\n",
    "}\n",
    "\n",
    "MAX_ITER = 1000#2**16\n",
    "MAX_EPOCHS = np.inf\n",
    "\n",
    "LOG_DIR = \"./log\"\n",
    "METAGRAPH_DIR = \"./out\"\n",
    "PLOTS_DIR = \"./png\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fully connected latent layers with sizes (?, 2)\n",
      "Created decoder layers\n",
      "created all layers\n",
      "var <tf.Variable 'enc_0/kernel:0' shape=(784, 500) dtype=float32_ref>\n",
      "grad Tensor(\"AdamOptimizer/gradients/enc_0/MatMul_grad/tuple/control_dependency_1:0\", shape=(784, 500), dtype=float32)\n",
      "var <tf.Variable 'enc_0/bias:0' shape=(500,) dtype=float32_ref>\n",
      "grad Tensor(\"AdamOptimizer/gradients/enc_0/BiasAdd_grad/tuple/control_dependency_1:0\", shape=(500,), dtype=float32)\n",
      "var <tf.Variable 'enc_1/kernel:0' shape=(500, 500) dtype=float32_ref>\n",
      "grad Tensor(\"AdamOptimizer/gradients/enc_1/MatMul_grad/tuple/control_dependency_1:0\", shape=(500, 500), dtype=float32)\n",
      "var <tf.Variable 'enc_1/bias:0' shape=(500,) dtype=float32_ref>\n",
      "grad Tensor(\"AdamOptimizer/gradients/enc_1/BiasAdd_grad/tuple/control_dependency_1:0\", shape=(500,), dtype=float32)\n",
      "var <tf.Variable 'enc_2/kernel:0' shape=(500, 100) dtype=float32_ref>\n",
      "grad Tensor(\"AdamOptimizer/gradients/enc_2/MatMul_grad/tuple/control_dependency_1:0\", shape=(500, 100), dtype=float32)\n",
      "var <tf.Variable 'enc_2/bias:0' shape=(100,) dtype=float32_ref>\n",
      "grad Tensor(\"AdamOptimizer/gradients/enc_2/BiasAdd_grad/tuple/control_dependency_1:0\", shape=(100,), dtype=float32)\n",
      "var <tf.Variable 'z_mean/kernel:0' shape=(100, 2) dtype=float32_ref>\n",
      "grad Tensor(\"AdamOptimizer/gradients/z_mean/MatMul_grad/tuple/control_dependency_1:0\", shape=(100, 2), dtype=float32)\n",
      "var <tf.Variable 'z_mean/bias:0' shape=(2,) dtype=float32_ref>\n",
      "grad Tensor(\"AdamOptimizer/gradients/z_mean/BiasAdd_grad/tuple/control_dependency_1:0\", shape=(2,), dtype=float32)\n",
      "var <tf.Variable 'z_log_sigma/kernel:0' shape=(100, 2) dtype=float32_ref>\n",
      "grad Tensor(\"AdamOptimizer/gradients/z_log_sigma/MatMul_grad/tuple/control_dependency_1:0\", shape=(100, 2), dtype=float32)\n",
      "var <tf.Variable 'z_log_sigma/bias:0' shape=(2,) dtype=float32_ref>\n",
      "grad Tensor(\"AdamOptimizer/gradients/z_log_sigma/BiasAdd_grad/tuple/control_dependency_1:0\", shape=(2,), dtype=float32)\n",
      "var <tf.Variable 'dec_2/kernel:0' shape=(2, 100) dtype=float32_ref>\n",
      "grad Tensor(\"AdamOptimizer/gradients/dec_2/MatMul_grad/tuple/control_dependency_1:0\", shape=(2, 100), dtype=float32)\n",
      "var <tf.Variable 'dec_2/bias:0' shape=(100,) dtype=float32_ref>\n",
      "grad Tensor(\"AdamOptimizer/gradients/dec_2/BiasAdd_grad/tuple/control_dependency_1:0\", shape=(100,), dtype=float32)\n",
      "var <tf.Variable 'dec_1/kernel:0' shape=(100, 500) dtype=float32_ref>\n",
      "grad Tensor(\"AdamOptimizer/gradients/dec_1/MatMul_grad/tuple/control_dependency_1:0\", shape=(100, 500), dtype=float32)\n",
      "var <tf.Variable 'dec_1/bias:0' shape=(500,) dtype=float32_ref>\n",
      "grad Tensor(\"AdamOptimizer/gradients/dec_1/BiasAdd_grad/tuple/control_dependency_1:0\", shape=(500,), dtype=float32)\n",
      "var <tf.Variable 'dec_0/kernel:0' shape=(500, 500) dtype=float32_ref>\n",
      "grad Tensor(\"AdamOptimizer/gradients/dec_0/MatMul_grad/tuple/control_dependency_1:0\", shape=(500, 500), dtype=float32)\n",
      "var <tf.Variable 'dec_0/bias:0' shape=(500,) dtype=float32_ref>\n",
      "grad Tensor(\"AdamOptimizer/gradients/dec_0/BiasAdd_grad/tuple/control_dependency_1:0\", shape=(500,), dtype=float32)\n",
      "var <tf.Variable 'decoder/kernel:0' shape=(500, 784) dtype=float32_ref>\n",
      "grad Tensor(\"AdamOptimizer/gradients/decoder/MatMul_grad/tuple/control_dependency_1:0\", shape=(500, 784), dtype=float32)\n",
      "var <tf.Variable 'decoder/bias:0' shape=(784,) dtype=float32_ref>\n",
      "grad Tensor(\"AdamOptimizer/gradients/decoder/BiasAdd_grad/tuple/control_dependency_1:0\", shape=(784,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "v = VariationalAutoEncoder(ARCHITECTURE, HYPERPARAMS, log_dir=LOG_DIR)\n",
    "#mnist = load_mnist()\n",
    "#v.train(mnist, max_iter=MAX_ITER, max_epochs=MAX_EPOCHS, cross_validate=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "Round 1, loss 69659.25\n",
      "Round 2, loss 70450.5234375\n",
      "Round 3, loss 69159.140625\n",
      "Round 4, loss 68784.28125\n",
      "Round 5, loss 68164.96875\n",
      "Round 6, loss 67217.25\n",
      "Round 7, loss 66044.25\n",
      "Round 8, loss 64623.35546875\n",
      "Round 9, loss 63187.015625\n",
      "Round 10, loss 60886.7578125\n",
      "Round 11, loss 58347.7890625\n",
      "Round 12, loss 55583.7421875\n",
      "Round 13, loss 52577.44921875\n",
      "Round 14, loss 49726.3125\n",
      "Round 15, loss 45560.890625\n",
      "Round 16, loss 42400.37109375\n",
      "Round 17, loss 39270.84765625\n",
      "Round 18, loss 37505.33203125\n",
      "Round 19, loss 34801.65234375\n",
      "Round 20, loss 32880.3125\n",
      "Round 21, loss 32675.9921875\n",
      "Round 22, loss 29679.1328125\n",
      "Round 23, loss 28839.7265625\n",
      "Round 24, loss 28829.921875\n",
      "Round 25, loss 29435.982421875\n",
      "Round 26, loss 29580.703125\n",
      "Round 27, loss 29025.48828125\n",
      "Round 28, loss 29010.28125\n",
      "Round 29, loss 28698.041015625\n",
      "Round 30, loss 28744.3203125\n",
      "Round 31, loss 28647.04296875\n",
      "Round 32, loss 27416.787109375\n",
      "Round 33, loss 27843.796875\n",
      "Round 34, loss 28130.36328125\n",
      "Round 35, loss 27003.142578125\n",
      "Round 36, loss 27385.359375\n",
      "Round 37, loss 27007.927734375\n",
      "Round 38, loss 27376.583984375\n",
      "Round 39, loss 27376.376953125\n",
      "Round 40, loss 26859.8984375\n",
      "Round 41, loss 26772.640625\n",
      "Round 42, loss 27540.744140625\n",
      "Round 43, loss 26983.376953125\n",
      "Round 44, loss 26630.869140625\n",
      "Round 45, loss 27474.65234375\n",
      "Round 46, loss 26856.82421875\n",
      "Round 47, loss 26847.4609375\n",
      "Round 48, loss 26883.486328125\n",
      "Round 49, loss 25291.27734375\n",
      "Round 50, loss 26035.8515625\n",
      "Round 51, loss 26875.11328125\n",
      "Round 52, loss 26357.84375\n",
      "Round 53, loss 26547.099609375\n",
      "Round 54, loss 26896.158203125\n",
      "Round 55, loss 26708.626953125\n",
      "Round 56, loss 26165.013671875\n",
      "Round 57, loss 25831.046875\n",
      "Round 58, loss 26714.078125\n",
      "Round 59, loss 27044.98828125\n",
      "Round 60, loss 26943.49609375\n",
      "Round 61, loss 26799.0546875\n",
      "Round 62, loss 26400.45703125\n",
      "Round 63, loss 26302.2421875\n",
      "Round 64, loss 26504.31640625\n",
      "Round 65, loss 26156.4453125\n",
      "Round 66, loss 26350.125\n",
      "Round 67, loss 26225.1328125\n",
      "Round 68, loss 26896.5546875\n",
      "Round 69, loss 26536.15234375\n",
      "Round 70, loss 26479.6640625\n",
      "Round 71, loss 26694.27734375\n",
      "Round 72, loss 26852.451171875\n",
      "Round 73, loss 26754.36328125\n",
      "Round 74, loss 26579.51171875\n",
      "Round 75, loss 26794.91796875\n",
      "Round 76, loss 27350.27734375\n",
      "Round 77, loss 26177.986328125\n",
      "Round 78, loss 27346.419921875\n",
      "Round 79, loss 25916.19140625\n",
      "Round 80, loss 26530.5625\n",
      "Round 81, loss 27194.12890625\n",
      "Round 82, loss 25871.77734375\n",
      "Round 83, loss 27008.1328125\n",
      "Round 84, loss 26215.25390625\n",
      "Round 85, loss 26769.20703125\n",
      "Round 86, loss 27413.76171875\n",
      "Round 87, loss 27623.17578125\n",
      "Round 88, loss 26621.548828125\n",
      "Round 89, loss 25928.38671875\n",
      "Round 90, loss 26865.1875\n",
      "Round 91, loss 26319.091796875\n",
      "Round 92, loss 26417.568359375\n",
      "Round 93, loss 26247.380859375\n",
      "Round 94, loss 26801.087890625\n",
      "Round 95, loss 25202.2734375\n",
      "Round 96, loss 26697.13671875\n",
      "Round 97, loss 26034.7265625\n",
      "Round 98, loss 26118.46875\n",
      "Round 99, loss 26486.7109375\n",
      "Round 100, loss 26952.224609375\n",
      "Round 101, loss 26832.046875\n",
      "Round 102, loss 26288.150390625\n",
      "Round 103, loss 27180.548828125\n",
      "Round 104, loss 26897.125\n",
      "Round 105, loss 27009.84765625\n",
      "Round 106, loss 27240.716796875\n",
      "Round 107, loss 26135.12109375\n",
      "Round 108, loss 26439.10546875\n",
      "Round 109, loss 26234.56640625\n",
      "Round 110, loss 26736.20703125\n",
      "Round 111, loss 26089.9140625\n",
      "Round 112, loss 26756.125\n",
      "Round 113, loss 26820.96484375\n",
      "Round 114, loss 26958.517578125\n",
      "Round 115, loss 26864.384765625\n",
      "Round 116, loss 26812.19140625\n",
      "Round 117, loss 26845.05859375\n",
      "Round 118, loss 26529.32421875\n",
      "Round 119, loss 27008.9609375\n",
      "Round 120, loss 26727.697265625\n",
      "Round 121, loss 26103.849609375\n",
      "Round 122, loss 25669.541015625\n",
      "Round 123, loss 26823.68359375\n",
      "Round 124, loss 26431.17578125\n",
      "Round 125, loss 26170.10546875\n",
      "Round 126, loss 27527.390625\n",
      "Round 127, loss 26892.6875\n",
      "Round 128, loss 27118.580078125\n",
      "Round 129, loss 26417.384765625\n",
      "Round 130, loss 26292.3515625\n",
      "Round 131, loss 26040.892578125\n",
      "Round 132, loss 26016.51171875\n",
      "Round 133, loss 26485.21484375\n",
      "Round 134, loss 26435.625\n",
      "Round 135, loss 26720.564453125\n",
      "Round 136, loss 26540.861328125\n",
      "Round 137, loss 26079.48828125\n",
      "Round 138, loss 26403.625\n",
      "Round 139, loss 25921.4375\n",
      "Round 140, loss 27185.0546875\n",
      "Round 141, loss 26511.201171875\n",
      "Round 142, loss 27133.05859375\n",
      "Round 143, loss 26248.6796875\n",
      "Round 144, loss 26771.98046875\n",
      "Round 145, loss 26409.55078125\n",
      "Round 146, loss 25820.03125\n",
      "Round 147, loss 26907.181640625\n",
      "Round 148, loss 26846.65234375\n",
      "Round 149, loss 27127.50390625\n",
      "Round 150, loss 27000.3828125\n",
      "Round 151, loss 26584.79296875\n",
      "Round 152, loss 26806.609375\n",
      "Round 153, loss 26950.265625\n",
      "Round 154, loss 26860.45703125\n",
      "Round 155, loss 27053.609375\n",
      "Round 156, loss 27102.806640625\n",
      "Round 157, loss 26680.658203125\n",
      "Round 158, loss 26730.576171875\n",
      "Round 159, loss 26338.75390625\n",
      "Round 160, loss 26287.498046875\n",
      "Round 161, loss 26784.115234375\n",
      "Round 162, loss 25807.82421875\n",
      "Round 163, loss 27132.611328125\n",
      "Round 164, loss 26518.5234375\n",
      "Round 165, loss 26960.16015625\n",
      "Round 166, loss 25351.81640625\n",
      "Round 167, loss 26084.4140625\n",
      "Round 168, loss 26590.068359375\n",
      "Round 169, loss 26490.294921875\n",
      "Round 170, loss 26378.33203125\n",
      "Round 171, loss 26566.591796875\n",
      "Round 172, loss 26204.296875\n",
      "Round 173, loss 26921.484375\n",
      "Round 174, loss 27117.91015625\n",
      "Round 175, loss 27180.947265625\n",
      "Round 176, loss 26527.259765625\n",
      "Round 177, loss 27078.01953125\n",
      "Round 178, loss 26228.634765625\n",
      "Round 179, loss 26762.400390625\n",
      "Round 180, loss 26125.7265625\n",
      "Round 181, loss 26704.90234375\n",
      "Round 182, loss 25932.47265625\n",
      "Round 183, loss 26300.369140625\n",
      "Round 184, loss 27635.365234375\n",
      "Round 185, loss 27434.640625\n",
      "Round 186, loss 26738.54296875\n",
      "Round 187, loss 26776.095703125\n",
      "Round 188, loss 27846.5\n",
      "Round 189, loss 26969.697265625\n",
      "Round 190, loss 26249.537109375\n",
      "Round 191, loss 26982.564453125\n",
      "Round 192, loss 26041.11328125\n",
      "Round 193, loss 26076.048828125\n",
      "Round 194, loss 26463.861328125\n",
      "Round 195, loss 26162.24609375\n",
      "Round 196, loss 26572.423828125\n",
      "Round 197, loss 26905.02734375\n",
      "Round 198, loss 25225.72265625\n",
      "Round 199, loss 27361.439453125\n",
      "Round 200, loss 26217.69921875\n",
      "Round 201, loss 25850.076171875\n",
      "Round 202, loss 26687.91796875\n",
      "Round 203, loss 26371.41015625\n",
      "Round 204, loss 28271.103515625\n",
      "Round 205, loss 25588.95703125\n",
      "Round 206, loss 26669.099609375\n",
      "Round 207, loss 26103.08984375\n",
      "Round 208, loss 25720.568359375\n",
      "Round 209, loss 26853.240234375\n",
      "Round 210, loss 26281.52734375\n",
      "Round 211, loss 26463.90234375\n",
      "Round 212, loss 26163.087890625\n",
      "Round 213, loss 26508.37890625\n",
      "Round 214, loss 26048.046875\n",
      "Round 215, loss 25749.03515625\n",
      "Round 216, loss 27433.720703125\n",
      "Round 217, loss 26833.72265625\n",
      "Round 218, loss 27007.240234375\n",
      "Round 219, loss 26743.25390625\n",
      "Round 220, loss 27574.08203125\n",
      "Round 221, loss 26454.91796875\n",
      "Round 222, loss 26143.96484375\n",
      "Round 223, loss 26964.16796875\n",
      "Round 224, loss 26160.60546875\n",
      "Round 225, loss 26243.984375\n",
      "Round 226, loss 27086.41796875\n",
      "Round 227, loss 26509.818359375\n",
      "Round 228, loss 27325.75390625\n",
      "Round 229, loss 25351.08984375\n",
      "Round 230, loss 27343.56640625\n",
      "Round 231, loss 25215.75390625\n",
      "Round 232, loss 26350.98828125\n",
      "Round 233, loss 26317.154296875\n",
      "Round 234, loss 27372.484375\n",
      "Round 235, loss 26950.03125\n",
      "Round 236, loss 27204.109375\n",
      "Round 237, loss 26511.78515625\n",
      "Round 238, loss 25633.10546875\n",
      "Round 239, loss 27065.87890625\n",
      "Round 240, loss 27375.923828125\n",
      "Round 241, loss 26178.48046875\n",
      "Round 242, loss 27314.85546875\n",
      "Round 243, loss 26869.595703125\n",
      "Round 244, loss 27534.58203125\n",
      "Round 245, loss 25948.951171875\n",
      "Round 246, loss 26131.98828125\n",
      "Round 247, loss 26254.23828125\n",
      "Round 248, loss 26355.75390625\n",
      "Round 249, loss 26516.171875\n",
      "Round 250, loss 26802.6484375\n",
      "Round 251, loss 26321.4765625\n",
      "Round 252, loss 26215.3125\n",
      "Round 253, loss 26834.13671875\n",
      "Round 254, loss 26953.072265625\n",
      "Round 255, loss 26208.380859375\n",
      "Round 256, loss 26681.9453125\n",
      "Round 257, loss 25729.32421875\n",
      "Round 258, loss 26939.953125\n",
      "Round 259, loss 27049.509765625\n",
      "Round 260, loss 26295.509765625\n",
      "Round 261, loss 25962.6796875\n",
      "Round 262, loss 26275.4921875\n",
      "Round 263, loss 27580.462890625\n",
      "Round 264, loss 26203.0234375\n",
      "Round 265, loss 26585.3828125\n",
      "Round 266, loss 26693.80078125\n",
      "Round 267, loss 26006.00390625\n",
      "Round 268, loss 26440.60546875\n",
      "Round 269, loss 26659.130859375\n",
      "Round 270, loss 26244.345703125\n",
      "Round 271, loss 26113.6875\n",
      "Round 272, loss 27071.515625\n",
      "Round 273, loss 27386.23828125\n",
      "Round 274, loss 26912.7421875\n",
      "Round 275, loss 42270.109375\n",
      "Round 276, loss 28069.63671875\n",
      "Round 277, loss 26851.962890625\n",
      "Round 278, loss 26329.62109375\n",
      "Round 279, loss 26295.470703125\n",
      "Round 280, loss 27434.40234375\n",
      "Round 281, loss 26967.712890625\n",
      "Round 282, loss 26263.8984375\n",
      "Round 283, loss 25769.734375\n",
      "Round 284, loss 26729.1640625\n",
      "Round 285, loss 26358.92578125\n",
      "Round 286, loss 25935.56640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 287, loss 26497.640625\n",
      "Round 288, loss 26271.177734375\n",
      "Round 289, loss 26525.17578125\n",
      "Round 290, loss 25608.91796875\n",
      "Round 291, loss 27094.21875\n",
      "Round 292, loss 27122.32421875\n",
      "Round 293, loss 26997.53125\n",
      "Round 294, loss 26637.51171875\n",
      "Round 295, loss 27616.046875\n",
      "Round 296, loss 26737.3203125\n",
      "Round 297, loss 26025.744140625\n",
      "Round 298, loss 26519.328125\n",
      "Round 299, loss 26582.81640625\n",
      "Round 300, loss 26878.1953125\n",
      "Round 301, loss 26526.8828125\n",
      "Round 302, loss 26893.51953125\n",
      "Round 303, loss 26949.4453125\n",
      "Round 304, loss 26016.3828125\n",
      "Round 305, loss 26072.85546875\n",
      "Round 306, loss 27123.76171875\n",
      "Round 307, loss 26226.201171875\n",
      "Round 308, loss 26160.40625\n",
      "Round 309, loss 27385.884765625\n",
      "Round 310, loss 26863.2578125\n",
      "Round 311, loss 26792.021484375\n",
      "Round 312, loss 25478.7734375\n",
      "Round 313, loss 27311.2734375\n",
      "Round 314, loss 27255.634765625\n",
      "Round 315, loss 26720.427734375\n",
      "Round 316, loss 26577.3671875\n",
      "Round 317, loss 26336.60546875\n",
      "Round 318, loss 26567.56640625\n",
      "Round 319, loss 26232.59375\n",
      "Round 320, loss 26926.65234375\n",
      "Round 321, loss 26585.99609375\n",
      "Round 322, loss 25810.6796875\n",
      "Round 323, loss 27242.123046875\n",
      "Round 324, loss 26503.5\n",
      "Round 325, loss 26658.765625\n",
      "Round 326, loss 26909.70703125\n",
      "Round 327, loss 26640.9296875\n",
      "Round 328, loss 26300.923828125\n",
      "Round 329, loss 25787.23046875\n",
      "Round 330, loss 27322.796875\n",
      "Round 331, loss 26560.78515625\n",
      "Round 332, loss 26267.83984375\n",
      "Round 333, loss 25794.17578125\n",
      "Round 334, loss 25574.775390625\n",
      "Round 335, loss 26566.73046875\n",
      "Round 336, loss 27158.2421875\n",
      "Round 337, loss 26359.046875\n",
      "Round 338, loss 26765.76171875\n",
      "Round 339, loss 26249.716796875\n",
      "Round 340, loss 26937.578125\n",
      "Round 341, loss 27020.689453125\n",
      "Round 342, loss 27719.22265625\n",
      "Round 343, loss 27021.169921875\n",
      "Round 344, loss 26990.486328125\n",
      "Round 345, loss 27004.39453125\n",
      "Round 346, loss 26523.6953125\n",
      "Round 347, loss 26721.615234375\n",
      "Round 348, loss 27676.0\n",
      "Round 349, loss 27292.046875\n",
      "Round 350, loss 26198.951171875\n",
      "Round 351, loss 27278.208984375\n",
      "Round 352, loss 25989.99609375\n",
      "Round 353, loss 26590.12890625\n",
      "Round 354, loss 26705.291015625\n",
      "Round 355, loss 25898.3203125\n",
      "Round 356, loss 26406.703125\n",
      "Round 357, loss 27420.982421875\n",
      "Round 358, loss 26286.986328125\n",
      "Round 359, loss 26185.8515625\n",
      "Round 360, loss 25684.54296875\n",
      "Round 361, loss 26289.51953125\n",
      "Round 362, loss 26030.716796875\n",
      "Round 363, loss 25975.54296875\n",
      "Round 364, loss 26912.677734375\n",
      "Round 365, loss 26860.365234375\n",
      "Round 366, loss 26466.01953125\n",
      "Round 367, loss 26953.140625\n",
      "Round 368, loss 25987.015625\n",
      "Round 369, loss 26887.853515625\n",
      "Round 370, loss 28274.109375\n",
      "Round 371, loss 26766.09375\n",
      "Round 372, loss 26741.17578125\n",
      "Round 373, loss 26368.2265625\n",
      "Round 374, loss 27520.3671875\n",
      "Round 375, loss 27341.01953125\n",
      "Round 376, loss 26652.341796875\n",
      "Round 377, loss 26554.421875\n",
      "Round 378, loss 26504.533203125\n",
      "Round 379, loss 26980.162109375\n",
      "Round 380, loss 26199.42578125\n",
      "Round 381, loss 26290.591796875\n",
      "Round 382, loss 27153.578125\n",
      "Round 383, loss 25802.50390625\n",
      "Round 384, loss 26250.2109375\n",
      "Round 385, loss 27376.416015625\n",
      "Round 386, loss 26049.396484375\n",
      "Round 387, loss 26241.837890625\n",
      "Round 388, loss 27012.18359375\n",
      "Round 389, loss 26890.861328125\n",
      "Round 390, loss 26703.11328125\n",
      "Round 391, loss 27171.166015625\n",
      "Round 392, loss 27019.109375\n",
      "Round 393, loss 26179.189453125\n",
      "Round 394, loss 26054.16015625\n",
      "Round 395, loss 27075.716796875\n",
      "Round 396, loss 26544.994140625\n",
      "Round 397, loss 26553.34375\n",
      "Round 398, loss 26705.14453125\n",
      "Round 399, loss 26427.419921875\n",
      "Round 400, loss 26337.23046875\n",
      "Round 401, loss 26022.59765625\n",
      "Round 402, loss 25560.984375\n",
      "Round 403, loss 26799.046875\n",
      "Round 404, loss 25490.08203125\n",
      "Round 405, loss 26811.35546875\n",
      "Round 406, loss 27481.42578125\n",
      "Round 407, loss 26429.73828125\n",
      "Round 408, loss 26954.833984375\n",
      "Round 409, loss 25650.419921875\n",
      "Round 410, loss 26530.455078125\n",
      "Round 411, loss 26582.404296875\n",
      "Round 412, loss 26344.658203125\n",
      "Round 413, loss 26980.58984375\n",
      "Round 414, loss 26454.587890625\n",
      "Round 415, loss 26889.72265625\n",
      "Round 416, loss 26295.67578125\n",
      "Round 417, loss 26472.30859375\n",
      "Round 418, loss 26364.755859375\n",
      "Round 419, loss 25872.71875\n",
      "Round 420, loss 26051.677734375\n",
      "Round 421, loss 26593.05078125\n",
      "Round 422, loss 26709.642578125\n",
      "Round 423, loss 26126.44921875\n",
      "Round 424, loss 26931.994140625\n",
      "Round 425, loss 25907.90625\n",
      "Round 426, loss 26382.8828125\n",
      "Round 427, loss 26212.09765625\n",
      "Round 428, loss 27057.7734375\n",
      "Round 429, loss 26692.1171875\n",
      "Round 430, loss 27594.806640625\n",
      "Round 431, loss 26926.4609375\n",
      "Round 432, loss 25958.6484375\n",
      "Round 433, loss 26028.052734375\n",
      "Round 434, loss 25501.796875\n",
      "Round 435, loss 27658.0546875\n",
      "Round 436, loss 27018.9375\n",
      "Round 437, loss 27432.904296875\n",
      "Round 438, loss 26336.509765625\n",
      "Round 439, loss 26502.85546875\n",
      "Round 440, loss 26860.67578125\n",
      "Round 441, loss 26553.375\n",
      "Round 442, loss 26704.505859375\n",
      "Round 443, loss 26942.486328125\n",
      "Round 444, loss 26596.51953125\n",
      "Round 445, loss 44907.74609375\n",
      "Round 446, loss 50776.796875\n",
      "Round 447, loss 28749.70703125\n",
      "Round 448, loss 26289.052734375\n",
      "Round 449, loss 25664.75390625\n",
      "Round 450, loss 26755.10546875\n",
      "Round 451, loss 27044.654296875\n",
      "Round 452, loss 27051.2734375\n",
      "Round 453, loss 27433.927734375\n",
      "Round 454, loss 26504.5625\n",
      "Round 455, loss 26536.01953125\n",
      "Round 456, loss 25485.421875\n",
      "Round 457, loss 26399.720703125\n",
      "Round 458, loss 25880.11328125\n",
      "Round 459, loss 30242.75390625\n",
      "Round 460, loss 25821.1328125\n",
      "Round 461, loss 27262.4140625\n",
      "Round 462, loss 26509.0625\n",
      "Round 463, loss 27112.828125\n",
      "Round 464, loss 26252.94921875\n",
      "Round 465, loss 26051.634765625\n",
      "Round 466, loss 34031.7734375\n",
      "Round 467, loss 39182.078125\n",
      "Round 468, loss 26236.34765625\n",
      "Round 469, loss 27224.12890625\n",
      "Round 470, loss 26657.82421875\n",
      "Round 471, loss 26723.849609375\n",
      "Round 472, loss 26601.314453125\n",
      "Round 473, loss 26212.16796875\n",
      "Round 474, loss 26689.54296875\n",
      "Round 475, loss 26450.19921875\n",
      "Round 476, loss 26850.623046875\n",
      "Round 477, loss 28400.14453125\n",
      "Round 478, loss 26656.5234375\n",
      "Round 479, loss 26139.076171875\n",
      "Round 480, loss 27078.48046875\n",
      "Round 481, loss 26448.14453125\n",
      "Round 482, loss 26084.328125\n",
      "Round 483, loss 27439.291015625\n",
      "Round 484, loss 26503.107421875\n",
      "Round 485, loss 27281.55078125\n",
      "Round 486, loss 27063.009765625\n",
      "Round 487, loss 27002.36328125\n",
      "Round 488, loss 26357.6484375\n",
      "Round 489, loss 26868.67578125\n",
      "Round 490, loss 27132.51171875\n",
      "Round 491, loss 26451.51953125\n",
      "Round 492, loss 26101.46875\n",
      "Round 493, loss 25676.0390625\n",
      "Round 494, loss 25746.76171875\n",
      "Round 495, loss 26428.884765625\n",
      "Round 496, loss 26310.51953125\n",
      "Round 497, loss 25649.236328125\n",
      "Round 498, loss 25676.33984375\n",
      "Round 499, loss 26292.978515625\n",
      "Round 500, loss 25564.197265625\n",
      "Round 501, loss 25764.60546875\n",
      "Round 502, loss 26229.236328125\n",
      "Round 503, loss 27011.80859375\n",
      "Round 504, loss 26059.91796875\n",
      "Round 505, loss 26203.55078125\n",
      "Round 506, loss 25929.1953125\n",
      "Round 507, loss 26376.46875\n",
      "Round 508, loss 27111.65625\n",
      "Round 509, loss 27023.56640625\n",
      "Round 510, loss 25497.15234375\n",
      "Round 511, loss 25217.49609375\n",
      "Round 512, loss 27012.326171875\n",
      "Round 513, loss 25410.4765625\n",
      "Round 514, loss 25412.943359375\n",
      "Round 515, loss 24988.40234375\n",
      "Round 516, loss 24949.306640625\n",
      "Round 517, loss 25997.96484375\n",
      "Round 518, loss 25692.787109375\n",
      "Round 519, loss 27432.4453125\n",
      "Round 520, loss 27093.732421875\n",
      "Round 521, loss 25069.421875\n",
      "Round 522, loss 25024.42578125\n",
      "Round 523, loss 25834.33203125\n",
      "Round 524, loss 26124.5\n",
      "Round 525, loss 24622.8828125\n",
      "Round 526, loss 26013.4765625\n",
      "Round 527, loss 24375.9296875\n",
      "Round 528, loss 24051.5078125\n",
      "Round 529, loss 24531.51953125\n",
      "Round 530, loss 25040.107421875\n",
      "Round 531, loss 26000.451171875\n",
      "Round 532, loss 24615.16015625\n",
      "Round 533, loss 24963.87109375\n",
      "Round 534, loss 25704.140625\n",
      "Round 535, loss 24821.72265625\n",
      "Round 536, loss 25673.078125\n",
      "Round 537, loss 25595.7890625\n",
      "Round 538, loss 25106.8671875\n",
      "Round 539, loss 26240.01171875\n",
      "Round 540, loss 24995.3828125\n",
      "Round 541, loss 24714.66796875\n",
      "Round 542, loss 25265.875\n",
      "Round 543, loss 25117.017578125\n",
      "Round 544, loss 24515.32421875\n",
      "Round 545, loss 25007.7578125\n",
      "Round 546, loss 24512.515625\n",
      "Round 547, loss 24462.224609375\n",
      "Round 548, loss 25177.00390625\n",
      "Round 549, loss 25084.09765625\n",
      "Round 550, loss 25056.92578125\n",
      "Round 551, loss 23814.83984375\n",
      "Round 552, loss 24808.87109375\n",
      "Round 553, loss 25085.056640625\n",
      "Round 554, loss 25024.07421875\n",
      "Round 555, loss 24611.455078125\n",
      "Round 556, loss 25032.685546875\n",
      "Round 557, loss 24705.671875\n",
      "Round 558, loss 24519.669921875\n",
      "Round 559, loss 24115.05859375\n",
      "Round 560, loss 25527.548828125\n",
      "Round 561, loss 25052.333984375\n",
      "Round 562, loss 25443.76171875\n",
      "Round 563, loss 24627.65234375\n",
      "Round 564, loss 25258.77734375\n",
      "Round 565, loss 24848.83203125\n",
      "Round 566, loss 24857.96484375\n",
      "Round 567, loss 24895.318359375\n",
      "Round 568, loss 24634.296875\n",
      "Round 569, loss 24309.56640625\n",
      "Round 570, loss 24715.740234375\n",
      "Round 571, loss 24815.3671875\n",
      "Round 572, loss 24811.80078125\n",
      "Round 573, loss 25031.931640625\n",
      "Round 574, loss 24353.05078125\n",
      "Round 575, loss 24390.7890625\n",
      "Round 576, loss 24638.796875\n",
      "Round 577, loss 24418.57421875\n",
      "Round 578, loss 24103.296875\n",
      "Round 579, loss 24320.92578125\n",
      "Round 580, loss 25802.185546875\n",
      "Round 581, loss 24231.796875\n",
      "Round 582, loss 25627.919921875\n",
      "Round 583, loss 24957.34375\n",
      "Round 584, loss 24927.38671875\n",
      "Round 585, loss 25090.580078125\n",
      "Round 586, loss 24543.5234375\n",
      "Round 587, loss 25081.359375\n",
      "Round 588, loss 24649.17578125\n",
      "Round 589, loss 23527.43359375\n",
      "Round 590, loss 23887.21875\n",
      "Round 591, loss 24238.09765625\n",
      "Round 592, loss 25042.025390625\n",
      "Round 593, loss 24701.626953125\n",
      "Round 594, loss 24665.8515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 595, loss 24509.314453125\n",
      "Round 596, loss 24929.775390625\n",
      "Round 597, loss 24833.185546875\n",
      "Round 598, loss 24587.853515625\n",
      "Round 599, loss 24877.3203125\n",
      "Round 600, loss 24294.810546875\n",
      "Round 601, loss 24532.26953125\n",
      "Round 602, loss 25133.591796875\n",
      "Round 603, loss 24091.9921875\n",
      "Round 604, loss 24583.57421875\n",
      "Round 605, loss 24473.16796875\n",
      "Round 606, loss 23850.115234375\n",
      "Round 607, loss 24648.30078125\n",
      "Round 608, loss 24154.12890625\n",
      "Round 609, loss 24606.8671875\n",
      "Round 610, loss 25596.916015625\n",
      "Round 611, loss 24915.779296875\n",
      "Round 612, loss 24793.12890625\n",
      "Round 613, loss 24839.966796875\n",
      "Round 614, loss 24664.685546875\n",
      "Round 615, loss 24453.3984375\n",
      "Round 616, loss 24486.00390625\n",
      "Round 617, loss 24044.138671875\n",
      "Round 618, loss 24342.734375\n",
      "Round 619, loss 23594.0703125\n",
      "Round 620, loss 23657.830078125\n",
      "Round 621, loss 24428.8515625\n",
      "Round 622, loss 24335.109375\n",
      "Round 623, loss 23972.080078125\n",
      "Round 624, loss 24100.341796875\n",
      "Round 625, loss 23682.1484375\n",
      "Round 626, loss 24393.91015625\n",
      "Round 627, loss 24630.4765625\n",
      "Round 628, loss 23987.923828125\n",
      "Round 629, loss 24025.39453125\n",
      "Round 630, loss 24412.216796875\n",
      "Round 631, loss 24746.193359375\n",
      "Round 632, loss 24946.64453125\n",
      "Round 633, loss 24685.07421875\n",
      "Round 634, loss 25007.55859375\n",
      "Round 635, loss 24045.923828125\n",
      "Round 636, loss 24439.74609375\n",
      "Round 637, loss 24313.4296875\n",
      "Round 638, loss 23587.4921875\n",
      "Round 639, loss 23886.390625\n",
      "Round 640, loss 24681.181640625\n",
      "Round 641, loss 24018.05859375\n",
      "Round 642, loss 23577.03515625\n",
      "Round 643, loss 25186.830078125\n",
      "Round 644, loss 24865.33984375\n",
      "Round 645, loss 24788.8984375\n",
      "Round 646, loss 24192.828125\n",
      "Round 647, loss 24650.35546875\n",
      "Round 648, loss 24259.16015625\n",
      "Round 649, loss 23937.0\n",
      "Round 650, loss 23665.419921875\n",
      "Round 651, loss 24869.525390625\n",
      "Round 652, loss 24330.103515625\n",
      "Round 653, loss 24400.4453125\n",
      "Round 654, loss 24544.7890625\n",
      "Round 655, loss 24947.84765625\n",
      "Round 656, loss 24444.765625\n",
      "Round 657, loss 24353.20703125\n",
      "Round 658, loss 23972.375\n",
      "Round 659, loss 24189.765625\n",
      "Round 660, loss 24864.09765625\n",
      "Round 661, loss 25306.3203125\n",
      "Round 662, loss 24458.443359375\n",
      "Round 663, loss 25046.857421875\n",
      "Round 664, loss 23723.25390625\n",
      "Round 665, loss 24829.650390625\n",
      "Round 666, loss 24863.46484375\n",
      "Round 667, loss 24382.25\n",
      "Round 668, loss 25156.3828125\n",
      "Round 669, loss 23511.96484375\n",
      "Round 670, loss 24310.984375\n",
      "Round 671, loss 23285.888671875\n",
      "Round 672, loss 24395.43359375\n",
      "Round 673, loss 24665.98828125\n",
      "Round 674, loss 24617.244140625\n",
      "Round 675, loss 24985.26171875\n",
      "Round 676, loss 24631.79296875\n",
      "Round 677, loss 24351.083984375\n",
      "Round 678, loss 24547.77734375\n",
      "Round 679, loss 24383.33203125\n",
      "Round 680, loss 25343.5703125\n",
      "Round 681, loss 23763.708984375\n",
      "Round 682, loss 24250.1328125\n",
      "Round 683, loss 24233.630859375\n",
      "Round 684, loss 25091.7578125\n",
      "Round 685, loss 24009.62890625\n",
      "Round 686, loss 24044.138671875\n",
      "Round 687, loss 24971.14453125\n",
      "Round 688, loss 24134.79296875\n",
      "Round 689, loss 24132.900390625\n",
      "Round 690, loss 24092.169921875\n",
      "Round 691, loss 24442.19921875\n",
      "Round 692, loss 24267.224609375\n",
      "Round 693, loss 23307.615234375\n",
      "Round 694, loss 23961.173828125\n",
      "Round 695, loss 23944.8984375\n",
      "Round 696, loss 24178.984375\n",
      "Round 697, loss 24828.94921875\n",
      "Round 698, loss 25539.94140625\n",
      "Round 699, loss 24938.140625\n",
      "Round 700, loss 24376.919921875\n",
      "Round 701, loss 23961.45703125\n",
      "Round 702, loss 23591.951171875\n",
      "Round 703, loss 25075.28515625\n",
      "Round 704, loss 23928.01171875\n",
      "Round 705, loss 24670.41015625\n",
      "Round 706, loss 24999.6171875\n",
      "Round 707, loss 24766.978515625\n",
      "Round 708, loss 23704.935546875\n",
      "Round 709, loss 25436.201171875\n",
      "Round 710, loss 23646.4765625\n",
      "Round 711, loss 24106.5234375\n",
      "Round 712, loss 23724.111328125\n",
      "Round 713, loss 25327.033203125\n",
      "Round 714, loss 23668.658203125\n",
      "Round 715, loss 23703.705078125\n",
      "Round 716, loss 25367.05078125\n",
      "Round 717, loss 23868.15234375\n",
      "Round 718, loss 24073.53125\n",
      "Round 719, loss 23705.23046875\n",
      "Round 720, loss 24370.158203125\n",
      "Round 721, loss 23698.013671875\n",
      "Round 722, loss 24552.2734375\n",
      "Round 723, loss 23294.62109375\n",
      "Round 724, loss 23943.328125\n",
      "Round 725, loss 24171.83984375\n",
      "Round 726, loss 23606.990234375\n",
      "Round 727, loss 24081.30859375\n",
      "Round 728, loss 24193.77734375\n",
      "Round 729, loss 24271.9609375\n",
      "Round 730, loss 24463.435546875\n",
      "Round 731, loss 24163.64453125\n",
      "Round 732, loss 24012.7265625\n",
      "Round 733, loss 24419.82421875\n",
      "Round 734, loss 23184.21875\n",
      "Round 735, loss 23992.16796875\n",
      "Round 736, loss 23758.65234375\n",
      "Round 737, loss 23994.119140625\n",
      "Round 738, loss 24927.66015625\n",
      "Round 739, loss 23874.681640625\n",
      "Round 740, loss 26311.0625\n",
      "Round 741, loss 23717.630859375\n",
      "Round 742, loss 23811.494140625\n",
      "Round 743, loss 24577.74609375\n",
      "Round 744, loss 24833.21875\n",
      "Round 745, loss 24158.3359375\n",
      "Round 746, loss 24655.990234375\n",
      "Round 747, loss 23730.33203125\n",
      "Round 748, loss 23742.353515625\n",
      "Round 749, loss 23950.17578125\n",
      "Round 750, loss 23942.87109375\n",
      "Round 751, loss 24436.763671875\n",
      "Round 752, loss 24706.60546875\n",
      "Round 753, loss 23779.517578125\n",
      "Round 754, loss 24415.521484375\n",
      "Round 755, loss 24269.298828125\n",
      "Round 756, loss 24034.984375\n",
      "Round 757, loss 24431.6328125\n",
      "Round 758, loss 24116.1640625\n",
      "Round 759, loss 22886.5859375\n",
      "Round 760, loss 24095.89453125\n",
      "Round 761, loss 23931.69921875\n",
      "Round 762, loss 23820.763671875\n",
      "Round 763, loss 23664.41015625\n",
      "Round 764, loss 24361.52734375\n",
      "Round 765, loss 24794.59765625\n",
      "Round 766, loss 24261.41015625\n",
      "Round 767, loss 24058.369140625\n",
      "Round 768, loss 23547.578125\n",
      "Round 769, loss 23777.5078125\n",
      "Round 770, loss 23091.09375\n",
      "Round 771, loss 24261.98046875\n",
      "Round 772, loss 22314.486328125\n",
      "Round 773, loss 23396.28515625\n",
      "Round 774, loss 24644.5625\n",
      "Round 775, loss 24322.470703125\n",
      "Round 776, loss 23335.76171875\n",
      "Round 777, loss 24163.548828125\n",
      "Round 778, loss 24178.626953125\n",
      "Round 779, loss 24089.150390625\n",
      "Round 780, loss 24341.4296875\n",
      "Round 781, loss 22673.66796875\n",
      "Round 782, loss 23652.91015625\n",
      "Round 783, loss 24507.7890625\n",
      "Round 784, loss 23999.705078125\n",
      "Round 785, loss 25000.29296875\n",
      "Round 786, loss 22972.65234375\n",
      "Round 787, loss 24373.578125\n",
      "Round 788, loss 24258.818359375\n",
      "Round 789, loss 22915.23828125\n",
      "Round 790, loss 24615.265625\n",
      "Round 791, loss 23829.16796875\n",
      "Round 792, loss 23671.703125\n",
      "Round 793, loss 22608.5859375\n",
      "Round 794, loss 23463.4375\n",
      "Round 795, loss 23570.30859375\n",
      "Round 796, loss 24129.4375\n",
      "Round 797, loss 24015.779296875\n",
      "Round 798, loss 24414.19140625\n",
      "Round 799, loss 23212.95703125\n",
      "Round 800, loss 23981.66796875\n",
      "Round 801, loss 23938.509765625\n",
      "Round 802, loss 24000.904296875\n",
      "Round 803, loss 25352.65234375\n",
      "Round 804, loss 23638.78125\n",
      "Round 805, loss 23310.51171875\n",
      "Round 806, loss 25341.0390625\n",
      "Round 807, loss 23842.228515625\n",
      "Round 808, loss 24280.00390625\n",
      "Round 809, loss 24626.51953125\n",
      "Round 810, loss 23814.67578125\n",
      "Round 811, loss 25187.0625\n",
      "Round 812, loss 23857.154296875\n",
      "Round 813, loss 23903.9140625\n",
      "Round 814, loss 24177.330078125\n",
      "Round 815, loss 24079.4140625\n",
      "Round 816, loss 24808.642578125\n",
      "Round 817, loss 23831.3046875\n",
      "Round 818, loss 23274.17578125\n",
      "Round 819, loss 24482.3671875\n",
      "Round 820, loss 23298.8125\n",
      "Round 821, loss 23925.24609375\n",
      "Round 822, loss 23638.244140625\n",
      "Round 823, loss 23719.55859375\n",
      "Round 824, loss 23718.888671875\n",
      "Round 825, loss 25058.49609375\n",
      "Round 826, loss 24119.876953125\n",
      "Round 827, loss 23486.59765625\n",
      "Round 828, loss 23842.685546875\n",
      "Round 829, loss 23861.583984375\n",
      "Round 830, loss 24395.318359375\n",
      "Round 831, loss 23861.056640625\n",
      "Round 832, loss 24110.91015625\n",
      "Round 833, loss 23399.88671875\n",
      "Round 834, loss 23663.67578125\n",
      "Round 835, loss 24019.634765625\n",
      "Round 836, loss 23397.0546875\n",
      "Round 837, loss 24791.689453125\n",
      "Round 838, loss 24395.9140625\n",
      "Round 839, loss 24308.90625\n",
      "Round 840, loss 24048.28515625\n",
      "Round 841, loss 24618.91796875\n",
      "Round 842, loss 24557.2578125\n",
      "Round 843, loss 24050.47265625\n",
      "Round 844, loss 23970.947265625\n",
      "Round 845, loss 24132.87890625\n",
      "Round 846, loss 23698.96484375\n",
      "Round 847, loss 24586.646484375\n",
      "Round 848, loss 23918.390625\n",
      "Round 849, loss 23154.3203125\n",
      "Round 850, loss 23453.4453125\n",
      "Round 851, loss 23708.51171875\n",
      "Round 852, loss 23738.125\n",
      "Round 853, loss 23496.5703125\n",
      "Round 854, loss 24045.5859375\n",
      "Round 855, loss 24152.4609375\n",
      "Round 856, loss 21842.859375\n",
      "Round 857, loss 23795.4296875\n",
      "Round 858, loss 23137.0078125\n",
      "Round 859, loss 24800.158203125\n",
      "Round 860, loss 24288.126953125\n",
      "Round 861, loss 24022.916015625\n",
      "Round 862, loss 22676.765625\n",
      "Round 863, loss 23972.3515625\n",
      "Round 864, loss 24572.28125\n",
      "Round 865, loss 23804.28125\n",
      "Round 866, loss 22507.78515625\n",
      "Round 867, loss 23436.16796875\n",
      "Round 868, loss 24168.572265625\n",
      "Round 869, loss 22644.55078125\n",
      "Round 870, loss 24454.9375\n",
      "Round 871, loss 23289.19921875\n",
      "Round 872, loss 23205.01171875\n",
      "Round 873, loss 23758.953125\n",
      "Round 874, loss 23048.2109375\n",
      "Round 875, loss 24132.943359375\n",
      "Round 876, loss 23056.25\n",
      "Round 877, loss 23519.990234375\n",
      "Round 878, loss 22955.099609375\n",
      "Round 879, loss 22791.37109375\n",
      "Round 880, loss 24230.0546875\n",
      "Round 881, loss 23115.29296875\n",
      "Round 882, loss 22970.07421875\n",
      "Round 883, loss 24388.0859375\n",
      "Round 884, loss 22458.40234375\n",
      "Round 885, loss 24041.28125\n",
      "Round 886, loss 24567.513671875\n",
      "Round 887, loss 24147.765625\n",
      "Round 888, loss 23341.607421875\n",
      "Round 889, loss 23659.44140625\n",
      "Round 890, loss 24236.748046875\n",
      "Round 891, loss 23990.1484375\n",
      "Round 892, loss 24617.29296875\n",
      "Round 893, loss 23624.162109375\n",
      "Round 894, loss 23586.94140625\n",
      "Round 895, loss 23069.28125\n",
      "Round 896, loss 23349.416015625\n",
      "Round 897, loss 23999.728515625\n",
      "Round 898, loss 24071.109375\n",
      "Round 899, loss 24400.615234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 900, loss 23554.30859375\n",
      "Round 901, loss 24145.943359375\n",
      "Round 902, loss 23819.71875\n",
      "Round 903, loss 24013.5859375\n",
      "Round 904, loss 23657.583984375\n",
      "Round 905, loss 24111.05078125\n",
      "Round 906, loss 23767.017578125\n",
      "Round 907, loss 23512.5234375\n",
      "Round 908, loss 23045.28515625\n",
      "Round 909, loss 23168.25390625\n",
      "Round 910, loss 23449.65625\n",
      "Round 911, loss 23035.01953125\n",
      "Round 912, loss 23073.03125\n",
      "Round 913, loss 23400.73046875\n",
      "Round 914, loss 24106.52734375\n",
      "Round 915, loss 23505.763671875\n",
      "Round 916, loss 23222.8984375\n",
      "Round 917, loss 23496.46484375\n",
      "Round 918, loss 23553.234375\n",
      "Round 919, loss 23533.17578125\n",
      "Round 920, loss 22541.248046875\n",
      "Round 921, loss 23327.1015625\n",
      "Round 922, loss 24094.244140625\n",
      "Round 923, loss 24507.13671875\n",
      "Round 924, loss 23461.275390625\n",
      "Round 925, loss 23512.546875\n",
      "Round 926, loss 23742.427734375\n",
      "Round 927, loss 23165.62109375\n",
      "Round 928, loss 24934.478515625\n",
      "Round 929, loss 23360.048828125\n",
      "Round 930, loss 23874.203125\n",
      "Round 931, loss 23911.59765625\n",
      "Round 932, loss 23247.099609375\n",
      "Round 933, loss 24025.46484375\n",
      "Round 934, loss 23858.35546875\n",
      "Round 935, loss 22871.71484375\n",
      "Round 936, loss 23364.41796875\n",
      "Round 937, loss 22904.63671875\n",
      "Round 938, loss 23624.625\n",
      "Round 939, loss 24367.86328125\n",
      "Round 940, loss 22946.140625\n",
      "Round 941, loss 23878.37890625\n",
      "Round 942, loss 24083.3828125\n",
      "Round 943, loss 23262.48046875\n",
      "Round 944, loss 24351.0\n",
      "Round 945, loss 22089.97265625\n",
      "Round 946, loss 23624.62890625\n",
      "Round 947, loss 24082.392578125\n",
      "Round 948, loss 23890.125\n",
      "Round 949, loss 25042.7421875\n",
      "Round 950, loss 23389.21484375\n",
      "Round 951, loss 23532.66015625\n",
      "Round 952, loss 23810.76171875\n",
      "Round 953, loss 24356.234375\n",
      "Round 954, loss 23460.78515625\n",
      "Round 955, loss 23843.337890625\n",
      "Round 956, loss 23057.93359375\n",
      "Round 957, loss 24458.638671875\n",
      "Round 958, loss 23294.6796875\n",
      "Round 959, loss 24485.544921875\n",
      "Round 960, loss 22553.765625\n",
      "Round 961, loss 23669.609375\n",
      "Round 962, loss 24135.796875\n",
      "Round 963, loss 24007.9453125\n",
      "Round 964, loss 23795.4765625\n",
      "Round 965, loss 23531.20703125\n",
      "Round 966, loss 23822.939453125\n",
      "Round 967, loss 23987.12109375\n",
      "Round 968, loss 23690.533203125\n",
      "Round 969, loss 23417.615234375\n",
      "Round 970, loss 23682.9609375\n",
      "Round 971, loss 23805.818359375\n",
      "Round 972, loss 22892.9375\n",
      "Round 973, loss 23270.5625\n",
      "Round 974, loss 23157.919921875\n",
      "Round 975, loss 23985.15625\n",
      "Round 976, loss 23198.9453125\n",
      "Round 977, loss 22981.77734375\n",
      "Round 978, loss 24360.3828125\n",
      "Round 979, loss 23973.53125\n",
      "Round 980, loss 23186.9609375\n",
      "Round 981, loss 23266.998046875\n",
      "Round 982, loss 23218.822265625\n",
      "Round 983, loss 23616.0625\n",
      "Round 984, loss 22967.2890625\n",
      "Round 985, loss 24608.052734375\n",
      "Round 986, loss 24714.68359375\n",
      "Round 987, loss 24221.0546875\n",
      "Round 988, loss 22030.140625\n",
      "Round 989, loss 24465.27734375\n",
      "Round 990, loss 23999.654296875\n",
      "Round 991, loss 23555.751953125\n",
      "Round 992, loss 23279.978515625\n",
      "Round 993, loss 22954.5234375\n",
      "Round 994, loss 24071.048828125\n",
      "Round 995, loss 23095.2265625\n",
      "Round 996, loss 23334.4140625\n",
      "Round 997, loss 23184.390625\n",
      "Round 998, loss 23359.2890625\n",
      "Round 999, loss 23735.826171875\n",
      "final avg cost (@ step 1000 = epoch 2): 26106.989759765624\n"
     ]
    }
   ],
   "source": [
    "mnist = load_mnist()\n",
    "v.train(mnist, max_iter=MAX_ITER, max_epochs=MAX_EPOCHS, cross_validate=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx,yy = mnist.train.next_batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f92726aabe0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIyJUGYJY5QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f927cd4ab70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xx[0].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f92725c7390>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCEsAMCAsAcCAsAQAA8ISAAwISwAw+D+2cXiwy7/WKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9272697080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xr = v.vae(xx)\n",
    "plt.imshow(xr[0].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'enc_0/kernel:0' shape=(784, 500) dtype=float32_ref>,\n",
       " <tf.Variable 'enc_0/bias:0' shape=(500,) dtype=float32_ref>,\n",
       " <tf.Variable 'enc_1/kernel:0' shape=(500, 500) dtype=float32_ref>,\n",
       " <tf.Variable 'enc_1/bias:0' shape=(500,) dtype=float32_ref>,\n",
       " <tf.Variable 'enc_2/kernel:0' shape=(500, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'enc_2/bias:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'z_mean/kernel:0' shape=(100, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'z_mean/bias:0' shape=(2,) dtype=float32_ref>,\n",
       " <tf.Variable 'z_log_sigma/kernel:0' shape=(100, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'z_log_sigma/bias:0' shape=(2,) dtype=float32_ref>,\n",
       " <tf.Variable 'dec_2/kernel:0' shape=(2, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'dec_2/bias:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'dec_1/kernel:0' shape=(100, 500) dtype=float32_ref>,\n",
       " <tf.Variable 'dec_1/bias:0' shape=(500,) dtype=float32_ref>,\n",
       " <tf.Variable 'dec_0/kernel:0' shape=(500, 500) dtype=float32_ref>,\n",
       " <tf.Variable 'dec_0/bias:0' shape=(500,) dtype=float32_ref>,\n",
       " <tf.Variable 'decoder/kernel:0' shape=(500, 784) dtype=float32_ref>,\n",
       " <tf.Variable 'decoder/bias:0' shape=(784,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enc_0/kernel:0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_t = tf.trainable_variables()[0]\n",
    "one_t.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss0 = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'enc_0/kernel/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'enc_1/kernel/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'enc_2/kernel/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'dec_2/kernel/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'dec_1/kernel/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'dec_0/kernel/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'decoder/kernel/Regularizer/l2_regularizer:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
